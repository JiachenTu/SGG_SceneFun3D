#!/usr/bin/env python3
"""
Scene Graph Output Validation Script

Validates the outputs generated by the scene graph pipeline to ensure
completeness, correctness, and quality.

Usage:
    python validate_outputs.py [--output-dir OUTPUT_DIR] [--verbose]
"""

import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple

import numpy as np


class SceneGraphValidator:
    """Validates scene graph outputs and data quality."""

    def __init__(self, output_dir: str = "outputs", verbose: bool = False):
        """Initialize validator."""
        self.output_dir = Path(output_dir)
        self.verbose = verbose
        self.validation_results = {
            'timestamp': datetime.now().isoformat(),
            'output_directory': str(self.output_dir),
            'checks': [],
            'summary': {}
        }

    def log(self, message: str, level: str = "INFO"):
        """Log a message."""
        if self.verbose:
            print(f"[{level}] {message}")

    def check_output_directory_structure(self) -> bool:
        """Check if output directory has the expected structure."""
        self.log("Checking output directory structure...")

        expected_dirs = [
            self.output_dir,
            self.output_dir / "scene_graphs",
            self.output_dir / "spatial_analysis",
            self.output_dir / "validation",
            self.output_dir / "logs"
        ]

        missing_dirs = []
        for directory in expected_dirs:
            if not directory.exists():
                missing_dirs.append(str(directory))

        check_result = {
            'check_name': 'output_directory_structure',
            'passed': len(missing_dirs) == 0,
            'expected_directories': [str(d) for d in expected_dirs],
            'missing_directories': missing_dirs,
            'message': "All expected directories exist" if len(missing_dirs) == 0 else f"Missing directories: {missing_dirs}"
        }

        self.validation_results['checks'].append(check_result)
        self.log(f"Directory structure check: {'PASSED' if check_result['passed'] else 'FAILED'}")

        return check_result['passed']

    def check_scene_graph_files(self) -> bool:
        """Check scene graph files for completeness and validity."""
        self.log("Checking scene graph files...")

        scene_graphs_dir = self.output_dir / "scene_graphs"
        if not scene_graphs_dir.exists():
            check_result = {
                'check_name': 'scene_graph_files',
                'passed': False,
                'message': "Scene graphs directory does not exist"
            }
            self.validation_results['checks'].append(check_result)
            return False

        scene_graph_files = list(scene_graphs_dir.glob("*.json"))

        check_result = {
            'check_name': 'scene_graph_files',
            'passed': True,
            'file_count': len(scene_graph_files),
            'files': [f.name for f in scene_graph_files],
            'file_validation': []
        }

        # Validate each scene graph file
        for file_path in scene_graph_files:
            file_validation = self.validate_scene_graph_file(file_path)
            check_result['file_validation'].append(file_validation)

            if not file_validation['valid']:
                check_result['passed'] = False

        check_result['message'] = f"Validated {len(scene_graph_files)} scene graph files"
        self.validation_results['checks'].append(check_result)

        self.log(f"Scene graph files check: {'PASSED' if check_result['passed'] else 'FAILED'}")
        return check_result['passed']

    def validate_scene_graph_file(self, file_path: Path) -> Dict[str, Any]:
        """Validate a single scene graph file."""
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)

            validation = {
                'filename': file_path.name,
                'valid': True,
                'issues': [],
                'statistics': {}
            }

            # Check required top-level fields
            required_fields = ['task_description', 'task_id', 'visit_id', 'video_id',
                             'nodes', 'root_id', 'target_affordances', 'spatial_reasoning_chain']

            for field in required_fields:
                if field not in data:
                    validation['issues'].append(f"Missing required field: {field}")
                    validation['valid'] = False

            # Check nodes structure
            if 'nodes' in data:
                nodes = data['nodes']
                validation['statistics']['node_count'] = len(nodes)

                # Count nodes by type
                node_types = {}
                for node_id, node in nodes.items():
                    if 'node_type' in node:
                        node_type = node['node_type']
                        node_types[node_type] = node_types.get(node_type, 0) + 1
                    else:
                        validation['issues'].append(f"Node {node_id} missing node_type")
                        validation['valid'] = False

                validation['statistics']['node_types'] = node_types

                # Check for required node types
                expected_types = ['scene_root', 'object', 'affordance']
                for expected_type in expected_types:
                    if expected_type not in node_types:
                        validation['issues'].append(f"Missing node type: {expected_type}")

            # Check target affordances
            if 'target_affordances' in data:
                target_count = len(data['target_affordances'])
                validation['statistics']['target_affordances_count'] = target_count

                if target_count == 0:
                    validation['issues'].append("No target affordances defined")

            # Check spatial reasoning chain
            if 'spatial_reasoning_chain' in data:
                reasoning_steps = len(data['spatial_reasoning_chain'])
                validation['statistics']['reasoning_steps'] = reasoning_steps

                if reasoning_steps == 0:
                    validation['issues'].append("Empty spatial reasoning chain")

            return validation

        except json.JSONDecodeError as e:
            return {
                'filename': file_path.name,
                'valid': False,
                'issues': [f"JSON decode error: {e}"],
                'statistics': {}
            }
        except Exception as e:
            return {
                'filename': file_path.name,
                'valid': False,
                'issues': [f"Validation error: {e}"],
                'statistics': {}
            }

    def check_spatial_analysis_files(self) -> bool:
        """Check spatial analysis output files."""
        self.log("Checking spatial analysis files...")

        spatial_dir = self.output_dir / "spatial_analysis"
        if not spatial_dir.exists():
            check_result = {
                'check_name': 'spatial_analysis_files',
                'passed': False,
                'message': "Spatial analysis directory does not exist"
            }
            self.validation_results['checks'].append(check_result)
            return False

        # Look for spatial relationships file
        relationships_file = spatial_dir / "spatial_relationships.json"

        check_result = {
            'check_name': 'spatial_analysis_files',
            'passed': relationships_file.exists(),
            'files_found': [f.name for f in spatial_dir.glob("*.json")],
            'relationships_file_exists': relationships_file.exists()
        }

        if relationships_file.exists():
            # Validate relationships file
            try:
                with open(relationships_file, 'r') as f:
                    relationships = json.load(f)

                check_result['relationship_count'] = len(relationships)
                check_result['high_confidence_count'] = sum(
                    1 for rel in relationships if rel.get('confidence', 0) > 0.7
                )
                check_result['message'] = f"Found {len(relationships)} spatial relationships"

            except Exception as e:
                check_result['passed'] = False
                check_result['message'] = f"Error reading relationships file: {e}"
        else:
            check_result['message'] = "Spatial relationships file not found"

        self.validation_results['checks'].append(check_result)
        self.log(f"Spatial analysis check: {'PASSED' if check_result['passed'] else 'FAILED'}")

        return check_result['passed']

    def check_data_consistency(self) -> bool:
        """Check consistency between input data and outputs."""
        self.log("Checking data consistency...")

        # Load pipeline summary if available
        summary_file = self.output_dir / "pipeline_summary.json"

        if not summary_file.exists():
            check_result = {
                'check_name': 'data_consistency',
                'passed': False,
                'message': "Pipeline summary file not found - cannot validate consistency"
            }
            self.validation_results['checks'].append(check_result)
            return False

        try:
            with open(summary_file, 'r') as f:
                summary = json.load(f)

            check_result = {
                'check_name': 'data_consistency',
                'passed': True,
                'issues': [],
                'input_stats': summary.get('input_statistics', {}),
                'output_stats': summary.get('processing_results', {})
            }

            # Check that all tasks have corresponding scene graphs
            input_tasks = check_result['input_stats'].get('scenefun3d_tasks', 0)
            output_graphs = check_result['output_stats'].get('scene_graphs_generated', 0)

            if input_tasks != output_graphs:
                check_result['issues'].append(
                    f"Task count mismatch: {input_tasks} input tasks, {output_graphs} output graphs"
                )
                check_result['passed'] = False

            # Check that relationships were found
            relationships_found = check_result['output_stats'].get('spatial_relationships_found', 0)
            if relationships_found == 0:
                check_result['issues'].append("No spatial relationships found")
                check_result['passed'] = False

            check_result['message'] = "Data consistency validated" if check_result['passed'] else "Data consistency issues found"

        except Exception as e:
            check_result = {
                'check_name': 'data_consistency',
                'passed': False,
                'message': f"Error checking consistency: {e}"
            }

        self.validation_results['checks'].append(check_result)
        self.log(f"Data consistency check: {'PASSED' if check_result['passed'] else 'FAILED'}")

        return check_result['passed']

    def generate_validation_summary(self) -> Dict[str, Any]:
        """Generate overall validation summary."""
        self.log("Generating validation summary...")

        total_checks = len(self.validation_results['checks'])
        passed_checks = sum(1 for check in self.validation_results['checks'] if check['passed'])

        self.validation_results['summary'] = {
            'total_checks': total_checks,
            'passed_checks': passed_checks,
            'failed_checks': total_checks - passed_checks,
            'overall_success': passed_checks == total_checks,
            'success_rate': passed_checks / total_checks if total_checks > 0 else 0
        }

        return self.validation_results['summary']

    def save_validation_report(self) -> str:
        """Save validation report to file."""
        validation_dir = self.output_dir / "validation"
        validation_dir.mkdir(exist_ok=True)

        report_file = validation_dir / f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(report_file, 'w') as f:
            json.dump(self.validation_results, f, indent=2)

        self.log(f"Validation report saved to: {report_file}")
        return str(report_file)

    def print_validation_summary(self):
        """Print validation summary to console."""
        summary = self.validation_results['summary']

        print("\n" + "=" * 60)
        print("VALIDATION SUMMARY")
        print("=" * 60)
        print(f"Total checks: {summary['total_checks']}")
        print(f"Passed: {summary['passed_checks']}")
        print(f"Failed: {summary['failed_checks']}")
        print(f"Success rate: {summary['success_rate']:.1%}")
        print(f"Overall result: {'PASSED' if summary['overall_success'] else 'FAILED'}")

        if not summary['overall_success']:
            print("\nFailed checks:")
            for check in self.validation_results['checks']:
                if not check['passed']:
                    print(f"  - {check['check_name']}: {check.get('message', 'No message')}")

        print("=" * 60)

    def run_validation(self) -> bool:
        """Run complete validation suite."""
        self.log("Starting validation suite...")

        # Run all validation checks
        checks = [
            self.check_output_directory_structure,
            self.check_scene_graph_files,
            self.check_spatial_analysis_files,
            self.check_data_consistency
        ]

        all_passed = True
        for check in checks:
            try:
                result = check()
                if not result:
                    all_passed = False
            except Exception as e:
                self.log(f"Error in validation check: {e}", "ERROR")
                all_passed = False

        # Generate summary
        self.generate_validation_summary()

        # Save report
        self.save_validation_report()

        # Print summary
        if self.verbose:
            self.print_validation_summary()

        return all_passed


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Validate Scene Graph Pipeline Outputs")
    parser.add_argument("--output-dir", default="outputs",
                        help="Output directory to validate (default: outputs)")
    parser.add_argument("--verbose", action="store_true",
                        help="Enable verbose output")

    args = parser.parse_args()

    # Check if output directory exists
    output_dir = Path(args.output_dir)
    if not output_dir.exists():
        print(f"ERROR: Output directory {output_dir} does not exist.")
        print("Run the pipeline first: python run_pipeline.py")
        sys.exit(1)

    # Create and run validator
    validator = SceneGraphValidator(
        output_dir=args.output_dir,
        verbose=args.verbose
    )

    success = validator.run_validation()

    if not args.verbose:
        validator.print_validation_summary()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()